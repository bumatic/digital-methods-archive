""" 
Script for importing a 4chan posts csv generated by 4CAT itself.
Useful to import datasets from other 4CAT instances.

psql command to export and compress a csv from 4CAT:
psql -d fourcat -c "COPY (SELECT * FROM posts_4chan WHERE board='BOARD') TO stdout WITH HEADER CSV DELIMITER ',';" | gzip > /path/file.csv.gz
"""

import argparse
import json
import time
import csv
import sys
import os
import re

from pathlib import Path

sys.path.insert(0, os.path.abspath(os.path.dirname(__file__)) + "/..")
from common.lib.database import Database
from common.lib.logger import Logger

# parse parameters
cli = argparse.ArgumentParser()
cli.add_argument("-i", "--input", required=True, help="File to read from, containing a CSV dump")
cli.add_argument("-d", "--datasource", type=str, required=True, help="Datasource ID")
cli.add_argument("-b", "--board", type=str, required=True, help="Board name")
cli.add_argument("-s", "--skip_duplicates", type=str, required=True, help="If duplicate posts should be skipped (useful if there's already data in the table)")
cli.add_argument("-o", "--offset", type=int, required=False, help="How many rows to skip")
args = cli.parse_args()

if not Path(args.input).exists() or not Path(args.input).is_file():
	print("%s is not a valid folder name." % args.input)
	sys.exit(1)

logger = Logger()
db = Database(logger=logger, appname="queue-dump")

csvnone = re.compile(r"^N$")

safe = False
if args.skip_duplicates:
	print("Skipping duplicate rows (ON CONFLICT DO NOTHING).")
	safe = True

with open(args.input, encoding="utf-8") as inputfile:
	fieldnames = ("id", "thread_id", "timestamp", "subject", "body", "author", "author_type", "author_type_id", "author_trip", "country_code", "country_name", "image_file", "image_4chan", "image_md5", "image_dimensions", "image_filesize", "semantic_url", "unsorted_data", "id_seq", "board", "image_url")

	reader = csv.DictReader(inputfile, fieldnames=fieldnames)
	
	# Skip headers
	next(reader, None)

	posts = 0
	threads = {}
	duplicates = 0

	# Show status
	if args.offset:
		print("Skipping %s rows." % args.offset)

	for post in reader:

		# We collect thread data first, even though we might skip this post
		if post["thread_id"] not in threads:
			threads[post["thread_id"]] = {
				"id": post["thread_id"],
				"board": args.board,
				"timestamp": 0,
				"timestamp_scraped": int(time.time()),
				"timestamp_modified": 0,
				"num_unique_ips": -1,
				"num_images": 0,
				"num_replies": 0,
				"limit_bump": False,
				"limit_image": False,
				"is_sticky": False,
				"is_closed": False,
				"post_last": 0
			}

			if post["thread_id"] == post["id"]:
				threads[post["thread_id"]]["timestamp"] = post["timestamp"]
				threads[post["thread_id"]]["is_sticky"] = False
				threads[post["thread_id"]]["is_closed"] = False

			if post["image_file"]:
				threads[post["thread_id"]]["num_images"] += 1

		threads[post["thread_id"]]["num_replies"] += 1
		threads[post["thread_id"]]["post_last"] = post["id"]
		threads[post["thread_id"]]["timestamp_modified"] = post["timestamp"]

		posts += 1

		# Skip rows if needed. Can be useful when importing didn't go correctly.
		if args.offset and posts < args.offset:
			continue

		# Collect post data
		post = {k: csvnone.sub("", post[k]) if post[k] else None for k in post}

		post_data = {
			"id": post["id"],
			"id_seq": post.get("id_seq", ""),
			"board": args.board,
			"thread_id": post["thread_id"],
			"timestamp": post.get("timestamp", ""),
			"subject": post.get("subject", ""),
			"body": post.get("body", ""),
			"author": post.get("author", ""),
			"author_type": post.get("author_type", ""),
			"author_type_id": post.get("author_type_id", "N"),
			"author_trip": post.get("author_trip", ""),
			"country_code": post.get("country_code", ""),
			"country_name": post.get("country_name", ""),
			"image_file": post.get("image_file", ""),
			"image_url": post.get("image_url", ""),
			"image_4chan": post.get("image_4chan", ""),
			"image_md5": post.get("image_md5", ""),
			"image_dimensions": post.get("image_dimensions", ""),
			"image_filesize": post.get("image_filesize", 0),
			"semantic_url": post.get("semantic_url", ""),
			"unsorted_data": post.get("unsorted_data", "")
		}

		db.insert("posts_4chan", post_data, commit=False, safe=safe)

		if posts > 0 and posts % 10000 == 0:
			print("Committing %i - %i post " % (posts - 10000, posts), end="")
			db.commit()

			# Commit threads that are at least two months older than the last encountered post. We use this gap to ensure thread data is up-to-date, even if the archive is only roughly ordered by time.
			# We do it this way to prevent RAM hogging.
			threads_added = set()
			for thread in threads.values():
				if (int(post["timestamp"]) - int(thread["timestamp_modified"])) > 5259487:
					db.insert("threads_4chan", data=thread, commit=False, safe=safe)
					threads_added.add(thread["id"])
			
			print("and %i threads" % len(threads_added), end="")
			for thread_added in threads_added:
				threads.pop(thread_added)
			print(" (%i threads waiting to commit)" % len(threads))
			db.commit()

	db.commit()

print("Done")